{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've saved the weights file in the same directory as your script\n",
    "weights_path = '/Users/gabeprice/Desktop/Blackwell Research 2024/Alzheimers Research/Alzheimer_s Photos/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "base_model = ResNet50(input_shape=(img_height, img_width, 3), include_top=False, weights=None)\n",
    "base_model.load_weights(weights_path)\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')  # Use 'softmax' for multi-class classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 176, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "img_path = '/Users/gabeprice/Desktop/Blackwell Research 2024/Alzheimers Research/Alzheimer_s Photos/train/NonDemented/nonDem5.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Get the shape of the image\n",
    "img_shape = img.shape\n",
    "print(img_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5121 images belonging to 4 classes.\n",
      "Found 1279 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 430ms/step - accuracy: 0.4616 - loss: 1.1036 - val_accuracy: 0.4968 - val_loss: 1.0586\n",
      "Epoch 2/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747us/step - accuracy: 0.5000 - loss: 0.9982 - val_accuracy: 0.6452 - val_loss: 0.8944\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 418ms/step - accuracy: 0.4906 - loss: 1.0590 - val_accuracy: 0.5000 - val_loss: 1.0426\n",
      "Epoch 4/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step - accuracy: 0.5625 - loss: 1.0088 - val_accuracy: 0.5161 - val_loss: 1.0447\n",
      "Epoch 5/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 411ms/step - accuracy: 0.4962 - loss: 1.0427 - val_accuracy: 0.5008 - val_loss: 1.0331\n",
      "Epoch 6/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - accuracy: 0.5938 - loss: 1.0159 - val_accuracy: 0.4839 - val_loss: 1.0323\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 19:39:00.220854: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 418ms/step - accuracy: 0.4990 - loss: 1.0423 - val_accuracy: 0.5232 - val_loss: 0.9717\n",
      "Epoch 8/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712us/step - accuracy: 0.4062 - loss: 1.0288 - val_accuracy: 0.6129 - val_loss: 0.7749\n",
      "Epoch 9/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 440ms/step - accuracy: 0.5065 - loss: 0.9954 - val_accuracy: 0.5481 - val_loss: 0.9526\n",
      "Epoch 10/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step - accuracy: 0.3750 - loss: 1.0655 - val_accuracy: 0.3871 - val_loss: 0.9739\n",
      "Epoch 11/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 525ms/step - accuracy: 0.5381 - loss: 0.9315 - val_accuracy: 0.5529 - val_loss: 0.9040\n",
      "Epoch 12/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747us/step - accuracy: 0.5938 - loss: 1.0102 - val_accuracy: 0.4516 - val_loss: 1.1706\n",
      "Epoch 13/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 413ms/step - accuracy: 0.5478 - loss: 0.9192 - val_accuracy: 0.5441 - val_loss: 0.9665\n",
      "Epoch 14/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step - accuracy: 0.5000 - loss: 0.8522 - val_accuracy: 0.5484 - val_loss: 0.9036\n",
      "Epoch 15/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 419ms/step - accuracy: 0.5587 - loss: 0.9121 - val_accuracy: 0.5617 - val_loss: 0.9047\n",
      "Epoch 16/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713us/step - accuracy: 0.5938 - loss: 0.9053 - val_accuracy: 0.4516 - val_loss: 1.1091\n",
      "Epoch 17/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 424ms/step - accuracy: 0.5615 - loss: 0.9111 - val_accuracy: 0.5545 - val_loss: 0.9013\n",
      "Epoch 18/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772us/step - accuracy: 0.4375 - loss: 0.9899 - val_accuracy: 0.5161 - val_loss: 0.8523\n",
      "Epoch 19/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 414ms/step - accuracy: 0.5552 - loss: 0.9242 - val_accuracy: 0.5561 - val_loss: 0.8928\n",
      "Epoch 20/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723us/step - accuracy: 0.6875 - loss: 0.7935 - val_accuracy: 0.5161 - val_loss: 0.8948\n",
      "Epoch 21/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 420ms/step - accuracy: 0.5489 - loss: 0.9083 - val_accuracy: 0.5553 - val_loss: 0.9112\n",
      "Epoch 22/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - accuracy: 0.5312 - loss: 0.9230 - val_accuracy: 0.5484 - val_loss: 0.8897\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 19:48:26.480152: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2104s\u001b[0m 13s/step - accuracy: 0.5212 - loss: 0.9463 - val_accuracy: 0.5393 - val_loss: 0.8897\n",
      "Epoch 24/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844us/step - accuracy: 0.4375 - loss: 0.9407 - val_accuracy: 0.5806 - val_loss: 0.7858\n",
      "Epoch 25/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 2s/step - accuracy: 0.5659 - loss: 0.8916 - val_accuracy: 0.5553 - val_loss: 0.9248\n",
      "Epoch 26/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step - accuracy: 0.6562 - loss: 0.7396 - val_accuracy: 0.4516 - val_loss: 0.9398\n",
      "Epoch 27/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 1s/step - accuracy: 0.5503 - loss: 0.8966 - val_accuracy: 0.5537 - val_loss: 0.9090\n",
      "Epoch 28/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step - accuracy: 0.4688 - loss: 0.9978 - val_accuracy: 0.4839 - val_loss: 1.1500\n",
      "Epoch 29/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 4s/step - accuracy: 0.5682 - loss: 0.8827 - val_accuracy: 0.5441 - val_loss: 0.8902\n",
      "Epoch 30/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 920us/step - accuracy: 0.5625 - loss: 0.9031 - val_accuracy: 0.5806 - val_loss: 0.7682\n",
      "Epoch 31/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1348s\u001b[0m 8s/step - accuracy: 0.5569 - loss: 0.8591 - val_accuracy: 0.5521 - val_loss: 0.8939\n",
      "Epoch 32/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - accuracy: 0.6562 - loss: 0.8103 - val_accuracy: 0.6452 - val_loss: 0.7238\n",
      "Epoch 33/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 568ms/step - accuracy: 0.5677 - loss: 0.8891 - val_accuracy: 0.5609 - val_loss: 0.9046\n",
      "Epoch 34/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695us/step - accuracy: 0.5000 - loss: 0.8879 - val_accuracy: 0.5806 - val_loss: 0.8215\n",
      "Epoch 35/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 773ms/step - accuracy: 0.5513 - loss: 0.8636 - val_accuracy: 0.5337 - val_loss: 0.9133\n",
      "Epoch 36/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686us/step - accuracy: 0.6562 - loss: 0.7847 - val_accuracy: 0.5484 - val_loss: 0.9137\n",
      "Epoch 37/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 568ms/step - accuracy: 0.5443 - loss: 0.8713 - val_accuracy: 0.5601 - val_loss: 0.8666\n",
      "Epoch 38/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step - accuracy: 0.4062 - loss: 1.0103 - val_accuracy: 0.4194 - val_loss: 1.0043\n",
      "Epoch 39/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 854ms/step - accuracy: 0.5660 - loss: 0.8543 - val_accuracy: 0.5505 - val_loss: 1.1331\n",
      "Epoch 40/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step - accuracy: 0.5312 - loss: 1.0252 - val_accuracy: 0.4839 - val_loss: 1.0294\n",
      "Epoch 41/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 850ms/step - accuracy: 0.5488 - loss: 0.8858 - val_accuracy: 0.5425 - val_loss: 0.8618\n",
      "Epoch 42/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714us/step - accuracy: 0.5312 - loss: 0.8996 - val_accuracy: 0.4839 - val_loss: 0.9211\n",
      "Epoch 43/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 5s/step - accuracy: 0.5630 - loss: 0.8656 - val_accuracy: 0.5649 - val_loss: 0.8916\n",
      "Epoch 44/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step - accuracy: 0.6875 - loss: 0.5529 - val_accuracy: 0.3871 - val_loss: 1.0872\n",
      "Epoch 45/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m956s\u001b[0m 6s/step - accuracy: 0.5737 - loss: 0.8544 - val_accuracy: 0.5625 - val_loss: 0.8726\n",
      "Epoch 46/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688us/step - accuracy: 0.5312 - loss: 0.9243 - val_accuracy: 0.6452 - val_loss: 0.7727\n",
      "Epoch 47/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 2s/step - accuracy: 0.5692 - loss: 0.8486 - val_accuracy: 0.5809 - val_loss: 0.8444\n",
      "Epoch 48/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799us/step - accuracy: 0.4062 - loss: 1.0119 - val_accuracy: 0.5806 - val_loss: 1.0084\n",
      "Epoch 49/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 475ms/step - accuracy: 0.5632 - loss: 0.8479 - val_accuracy: 0.5681 - val_loss: 0.8500\n",
      "Epoch 50/50\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 770us/step - accuracy: 0.5938 - loss: 0.9671 - val_accuracy: 0.6774 - val_loss: 0.8371\n",
      "40/40 - 4s - 105ms/step - accuracy: 0.5794 - loss: 0.8549\n",
      "Test accuracy: 0.5793588757514954\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define paths to your dataset\n",
    "train_data_dir = '/Users/gabeprice/Desktop/Blackwell Research 2024/Alzheimers Research/Alzheimer_s Photos/train'\n",
    "test_data_dir = '/Users/gabeprice/Desktop/Blackwell Research 2024/Alzheimers Research/Alzheimer_s Photos/test'\n",
    "img_height, img_width = 176, 208  # Adjust according to your image dimensions\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator instances for training and validation data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Since you have multiple classes\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')  # 4 classes: MildDemented, ModerateDemented, NonDemented, VeryMildDemented\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(validation_generator, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
